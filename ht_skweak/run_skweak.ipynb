{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from labeling_functions import RuleAnnotator, DictionaryAnnotator, NameDisambiguationAnnotator, FrequencyDetector\n",
    "from labeling_functions import AllCapsDetector, NameCaseStructureDetector, combine_lfs\n",
    "\n",
    "from neat_preprocess import preprocess\n",
    "from process_doc import create_doc, get_docs, store_doc_list\n",
    "\n",
    "from skweak.generative import HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to resources\n",
    "\n",
    "data_path = \"src/HTName.csv\"\n",
    "\n",
    "namelist_path = \"src/nameslist.csv\"\n",
    "dictionary_path = \"src/weights.json\"\n",
    "expanded_dictionary_path = \"/src/generatedFemaleNamesPlusOriginalDict30000 (1).json\"\n",
    "\n",
    "model_path = \"ht_bert_v3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 1. Process data into spacy docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"HI MIA HERE  FIRST TIME IN THIS CITY,WOULD LIKE TO MEET NICE GUYS...  COME MEET ME TO HAVE UNFORGETTHABLE TIME TOGETHER...NEVER RUSH  OPEN - MINDED MENU  CALL TEXT... 6472055427 EGLINTON AVE E SCARBOROUGH\"]\n",
    "text = list(map(preprocess, text))\n",
    "\n",
    "# load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# create docs\n",
    "data_docs = create_doc(text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lf\n",
    "lfs = []\n",
    "\n",
    "# NEAT LFs\n",
    "# 0 - don't use, 1 - use, 2 - threshold (if applicable)\n",
    "rule = 1\n",
    "dictionary = 0\n",
    "exp_dictionary = 1\n",
    "disambiguation = 0\n",
    "cap_disambiguation = 0\n",
    "frequency = 0\n",
    "all_caps = 0\n",
    "name_structure = 0\n",
    "\n",
    "if rule == 1:\n",
    "  for i in range(27):\n",
    "    lfs.append(RuleAnnotator(i))\n",
    "\n",
    "if dictionary == 1:\n",
    "  lfs = lfs + [DictionaryAnnotator(dictionary_path, \"full_dictionary\")]\n",
    "elif dictionary == 2:\n",
    "  thresholds = [\"q1\", \"q2\", \"q3\", \"q4\"]\n",
    "  for threshold in thresholds:\n",
    "    lfs.append(DictionaryAnnotator(dictionary_path, threshold + \"_thr_dictionary\"))\n",
    "\n",
    "if exp_dictionary == 1:\n",
    "  lfs = lfs + [DictionaryAnnotator(dictionary_path, \"full_expanded_dictionary\")]\n",
    "elif exp_dictionary == 2:\n",
    "  thresholds = [\"q1\", \"q2\", \"q3\", \"q4\"]\n",
    "  for threshold in thresholds:\n",
    "    lfs.append(DictionaryAnnotator(dictionary_path, threshold + \"_thr_expanded_dictionary\"))\n",
    "\n",
    "if disambiguation == 1:\n",
    "  lfs = lfs + [NameDisambiguationAnnotator(thr = 0.1, add_bound = 0.05, upper_bound = False, weights_dict_path = sources_path + \"/full_dictionary.json\")]\n",
    "elif disambiguation == 2:\n",
    "  thresholds = [0.1, 0.2, 0.3, 0.4]\n",
    "  for threshold in thresholds:\n",
    "    lfs.append(NameDisambiguationAnnotator(thr = threshold, add_bound = 0.05, upper_bound = False, weights_dict_path = sources_path + \"/full_dictionary.json\"))\n",
    "\n",
    "# Extra Rule LFs\n",
    "\n",
    "if frequency > 1:\n",
    "  # all tokens that arent stop words or punctuations\n",
    "  words = []\n",
    "  for doc in tqdm(data_docs):  # for doc in data we want to fit hmm on\n",
    "    words = words + [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "  # get most common tokens\n",
    "  word_freq = Counter(words)\n",
    "  sorted_word_freq = [x[0] for x in word_freq.most_common(len(word_freq))]\n",
    "\n",
    "  # threshold frequency detector\n",
    "  if frequency == 1:\n",
    "    lfs.append(FrequencyDetector(sorted_word_freq, 0.01))\n",
    "  elif frequency == 2:\n",
    "    thresholds = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "    for threshold in thresholds:\n",
    "      lfs.append(FrequencyDetector(sorted_word_freq, threshold))\n",
    "    \n",
    "# capital detectors\n",
    "if all_caps == 1:\n",
    "  lfs.append(AllCapsDetector())\n",
    "\n",
    "if name_structure == 1:\n",
    "  lfs.append(NameCaseStructureDetector())\n",
    "\n",
    "# combine annotators\n",
    "combined_annotator = combine_lfs(lfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_docs = []\n",
    "for doc in tqdm(data_docs):\n",
    "    combine_lfs(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_model = HMM(\"hmm\", labels = [\"PERSON_NAME\", \"NOT_NAME\"])\n",
    "unified_model.fit(annotated_docs)\n",
    "\n",
    "unified_docs = []\n",
    "for doc in tqdm(annotated_docs):\n",
    "  unified_docs.append(unified_model(doc))\n",
    "\n",
    "for doc in unified_docs:\n",
    "    doc.ents = doc.spans[\"hmm\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy docs -> list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_preds = []  # list of results\n",
    "for doc in unified_docs:\n",
    "  entities = ''\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON_NAME\":\n",
    "      entities+=ent.text+\"|\"\n",
    "  if entities != '':\n",
    "    hmm_preds.append(entities)\n",
    "  else:\n",
    "    hmm_preds.append('N')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
